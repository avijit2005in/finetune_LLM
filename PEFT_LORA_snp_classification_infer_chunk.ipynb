{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n5wMMbtj96Pe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python39/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"BioMistral/BioMistral-7B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ty2PMZOV994s"
   },
   "outputs": [],
   "source": [
    "#Infer RGCIC Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "classified_krishna_df = pd.read_csv('/home/necuser/sdp/Classification/LLM/Finetuining/data/train/processed_chunked_files_llm_finetuining_krishna.csv')\n",
    "classified_sandhya_df = pd.read_csv('/home/necuser/sdp/Classification/LLM/Finetuining/data/train/processed_chunked_files_llm_finetuining_sandhya.csv')\n",
    "\n",
    "classified_df = pd.concat([classified_sandhya_df, classified_krishna_df], axis=0, ignore_index=True)\n",
    "dataset = Dataset.from_pandas(classified_df)\n",
    "dataset = dataset.train_test_split(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Infer Shrishti Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "classified_shrishti_df = pd.read_csv('/home/jupyter/finetune_LLM/Data/shrishti/train/processed_chunked_files_llm_finetuining_shrishti.csv')\n",
    "\n",
    "classified_df = classified_shrishti_df\n",
    "dataset = Dataset.from_pandas(classified_df)\n",
    "dataset = dataset.train_test_split(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AHr6QlkF-F6O"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['filename', 'notes', 'classified'],\n",
       "        num_rows: 9468\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['filename', 'notes', 'classified'],\n",
       "        num_rows: 1053\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Notes_text_shrishti_chunk_7024.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']['filename'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrXhx4ne-Gef"
   },
   "outputs": [],
   "source": [
    "# Find the index of the target string - RGCIC\n",
    "target_string = \"Notes_text_7202_chunk_1.txt\"\n",
    "string_list = dataset[\"test\"][\"filename\"]\n",
    "if target_string in string_list:\n",
    "    index = string_list.index(target_string)\n",
    "    print(f\"The index of '{target_string}' is: {index}\")\n",
    "else:\n",
    "    print(f\"The string '{target_string}' is not in the list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of 'Notes_text_shrishti_chunk_7024.txt' is: 0\n"
     ]
    }
   ],
   "source": [
    "# Find the index of the target string - Shrishti\n",
    "target_string = \"Notes_text_shrishti_chunk_7024.txt\"\n",
    "string_list = dataset[\"test\"][\"filename\"]\n",
    "if target_string in string_list:\n",
    "    index = string_list.index(target_string)\n",
    "    print(f\"The index of '{target_string}' is: {index}\")\n",
    "else:\n",
    "    print(f\"The string '{target_string}' is not in the list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4FKe0aSI-IlC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/finetune_LLM'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lgzUTMBp-Key"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel\n",
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2DQZ12H0-MSf"
   },
   "outputs": [],
   "source": [
    "#adapter = \"biomistral-snp-finetune-classifier-2024_130624_v2/checkpoint-1000\" #RGCIC\n",
    "adapter = \"finetuned_LLMbiomistral-snp-finetune-shrishti-classifier-2024_030724_v1/checkpoint-1000\" #Shrishti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HR5bJi3c-On8"
   },
   "outputs": [],
   "source": [
    "#Load and activate the adapter on top of the base model\n",
    "ft_model_fast = PeftModel.from_pretrained(base_model, adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_pbXO7Mr-Rch"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python39/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Merge the adapter with the base model\n",
    "ft_model_fast = ft_model_fast.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "po_vxmws-TPP"
   },
   "outputs": [],
   "source": [
    "#Save the merged model in a directory in the safetensors format\n",
    "model_dir = \"./biomistral-snp-finetune-classifier-2024_130624_v2/merged_model/\"\n",
    "ft_model_fast.save_pretrained(model_dir, safe_serialization=True)\n",
    "\n",
    "#Save the custom tokenizer in the same directory\n",
    "eval_tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKW143gl-XHe"
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=\"./biomistral-snp-finetune-classifier-2024_130624_v2/merged_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TLlmj4j-aQp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qADeG8z4-aXJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "at2wHvja-abj"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "#ft_model = PeftModel.from_pretrained(base_model, \"biomistral-snp-finetune-classifier-2024_130624_v2/checkpoint-1000\") #RGCIC\n",
    "ft_model = PeftModel.from_pretrained(base_model, adapter) #Shrishti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxvnqZSl-doJ"
   },
   "outputs": [],
   "source": [
    "#RGCIC\n",
    "eval_prompt = f\"\"\"Classify the notes into below categories:\n",
    "                   'allergies',\n",
    "                   'chief_complaints',\n",
    "                   'diagnosis',\n",
    "                   'family_history',\n",
    "                   'history',\n",
    "                   'instructions_advice',\n",
    "                   'investigation_report',\n",
    "                   'investigations',\n",
    "                   'medicine_prescription',\n",
    "                   'observations_examinations',\n",
    "                   'patient_willingness_concent',\n",
    "                   'personal_history',\n",
    "                   'procedure_report',\n",
    "                   'referral',\n",
    "                   'social_history',\n",
    "                   'tolerance',\n",
    "                   'treatment_plan',\n",
    "                   'unclassified',\n",
    "                   'vitals'\n",
    "                    Output the classified data into json format\n",
    "\n",
    "\n",
    "### notes:\n",
    "{dataset[\"test\"][\"notes\"][14]}\n",
    "\n",
    "### Classified:\n",
    "\"\"\"\n",
    "print(eval_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the notes into below categories:\n",
      "                   'recipere', \n",
      "                   'investigations', \n",
      "                   'plan', \n",
      "                   'complaints',\n",
      "                   'history_of_previous_illness', \n",
      "                   'examination', \n",
      "                   'diagnoses',\n",
      "                    Output the classified data into json format\n",
      "\n",
      "\n",
      "### notes:\n",
      "oefgc, vitals bp 14485 pr 66 spo2 99% temp 35.9 rbs 7.2 mmollcvs s1, s2 heard no murmursresp bilateral vesicular breath soundspa normal scaphoid abdomenno bipedal edemacns gcs1515, pupils berl, no neurological deficits\n",
      "\n",
      "### Classified:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Shrishti\n",
    "eval_prompt = f\"\"\"Classify the notes into below categories:\n",
    "                   'recipere', \n",
    "                   'investigations', \n",
    "                   'plan', \n",
    "                   'complaints',\n",
    "                   'history_of_previous_illness', \n",
    "                   'examination', \n",
    "                   'diagnoses',\n",
    "                    Output the classified data into json format\n",
    "\n",
    "\n",
    "### notes:\n",
    "{dataset[\"test\"][\"notes\"][1]}\n",
    "\n",
    "### Classified:\n",
    "\"\"\"\n",
    "print(eval_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sZ00eQq1-frK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'recipere': ['terbutaline+ambroxol+guaifensine syrup, dosage: 1, route: 106.0, qty: 1.0, duration: 1.0, dura_unit: 193.0, instructions: . ibuprofen+paracetamol (125+100)mg/5ml suspension 100ml, dosage: 1, route: 106.0, qty: 1.0, duration: 5.0, dura_unit: 193.0, instructions: take 5ml thrice daily cefuroxime 250mg/5ml suspension, dosage: 1, route: 106.0, qty: 1.0, duration: 5.0, dura_unit: 193.0, instructions: take 5ml twice daily paracetamol injection 10ml, dosage: 1, route: 106.0, qty: 1.0, duration: 1.0, dura_unit: 193.0, instructions: . diphenhydramine+sodium citrate+menthol (7+28.5+0.55)mg/5ml syrup 100ml, dosage: 1, route: 106.0, qty: 1.0, duration: 5.0, dura_unit: 193.0, instructions: take 5ml thrice daily ebastine 10mg tablets, dosage: 1, route: 106.0, qty: 1.0, duration: 1.0, dura_unit: 194.0, instructions: take 5ml at night'], 'complaints': ['Persistent cough with chest pain. Loss of voice. Hot flashes with chills. Headaches.', 'Abdominal pain, fever, diarrhea', 'Headache, sore throat, runny nose, abdominal pain, nausea, vomiting, loose stools multiple times, loss of appetite and fatigue, no travel, no urinary symptoms.', 'joint pains, general body weakness', 'Mucous stool and abdominal pain'], 'examination': ['In FGC, palpate epigastric tenderness.', 'FBC and general examination are normal. Entrance and respiration are normal. Chest is clear. Local mild pustular skin rashes over neck (nape) and hands and legs noted. No whitish coating. Mild excoriation markings. No vesicles seen. Other systemic examination is normal.', 'Fingers and toes were not pale, jaundiced, cyanosed, and there were bilateral scattered rhonchi heard on respiration.', 'FGCPa soft with epigastric tenderness', 'In generalized tenderness with no organomegaly.'], 'history_of_previous_illness': ['Hypertensive on Aldomet BDadalat 20mg BD', 'Dry cough, especially in the evening, during the day she is okay. Premedscalpol, vomiting, good appetite, normal bowel motions. No history of recent travel.', 'No cough and oral sores'], 'investigations': ['Complete Blood Count, Helicobacter pylori Antigen Screening, Helicobacter pylori Antigen Screening', 'Stool microscopy, Random Blood Sugar, Complete Blood Count, Urinalysis, Prostate Specific Antigen (Total), Occult Blood in Stool, Salmonella Antigen Test, Helicobacter pylori Antigen Screening']}\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][\"classified\"][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUBmqnCn-ibj"
   },
   "outputs": [],
   "source": [
    "dataset[\"test\"]['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jQYLRcXy-lFe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1053"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WZFjQC21-nb3"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "#ft_model = PeftModel.from_pretrained(base_model, \"biomistral-snp-finetune-classifier-2024_130624_v2/checkpoint-1000\") #RGCIC\n",
    "ft_model = PeftModel.from_pretrained(base_model, adapter) #Shrishti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezpg7Hcz-pb7"
   },
   "outputs": [],
   "source": [
    "#RGCIC\n",
    "def prompt_func(i):\n",
    "    eval_prompt = f\"\"\"Classify the notes into below categories:\n",
    "                   'allergies',\n",
    "                   'chief_complaints',\n",
    "                   'diagnosis',\n",
    "                   'family_history',\n",
    "                   'history',\n",
    "                   'instructions_advice',\n",
    "                   'investigation_report',\n",
    "                   'investigations',\n",
    "                   'medicine_prescription',\n",
    "                   'observations_examinations',\n",
    "                   'patient_willingness_concent',\n",
    "                   'personal_history',\n",
    "                   'procedure_report',\n",
    "                   'referral',\n",
    "                   'social_history',\n",
    "                   'tolerance',\n",
    "                   'treatment_plan',\n",
    "                   'unclassified',\n",
    "                   'vitals'\n",
    "                    Output the classified data into json format\n",
    "\n",
    "    ### notes:\n",
    "    {dataset[\"test\"][\"notes\"][i]}\n",
    "\n",
    "    ### Classified:\n",
    "    \"\"\"\n",
    "    return eval_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shrishti\n",
    "def prompt_func(i):\n",
    "    eval_prompt = f\"\"\"Classify the notes into below categories:\n",
    "                   'recipere', \n",
    "                   'investigations', \n",
    "                   'plan', \n",
    "                   'complaints',\n",
    "                   'history_of_previous_illness', \n",
    "                   'examination', \n",
    "                   'diagnoses',\n",
    "                    Output the classified data into json format\n",
    "\n",
    "\n",
    "    ### notes:\n",
    "    {dataset[\"test\"][\"notes\"][i]}\n",
    "\n",
    "    ### Classified:\n",
    "    \"\"\"\n",
    "    return eval_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7-ffzpq1-sIH"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def ft_model_func(eval_prompt):\n",
    "    model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    ft_model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = ft_model.generate(\n",
    "            **model_input,\n",
    "            max_new_tokens=1100, #1048,\n",
    "            num_beams=5,\n",
    "            temperature=0.0,\n",
    "            top_k=10,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=2.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    decoded_outputs = eval_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    match = re.search(r'### Classified:\\s*({.*?})\\s*(?=\\n|$)', decoded_outputs[0], re.DOTALL)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df_long = pd.DataFrame()\n",
    "\n",
    "    if match:\n",
    "        classified_data = match.group(1)\n",
    "        print(\"Extracted Data:\", classified_data)\n",
    "\n",
    "        classified_dict = json.loads(classified_data.replace(\"'\", '\"'))\n",
    "        print(\"Dictionary Format:\", classified_dict)\n",
    "\n",
    "        df = pd.DataFrame.from_dict({key: pd.Series(value) for key, value in classified_dict.items()})\n",
    "        #print(df)\n",
    "    else:\n",
    "        print(\"No classified data found.\")\n",
    "\n",
    "    if not df.empty:\n",
    "\n",
    "        df_long = df.melt(var_name='Label', value_name='Text', value_vars=df.columns)\n",
    "\n",
    "        df_long.dropna(subset=['Text'], inplace=True)\n",
    "\n",
    "        print(\"\\nTransformed DataFrame:\")\n",
    "        df_long.reset_index(drop=True, inplace=True)\n",
    "        print(df_long)\n",
    "\n",
    "    return df_long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "0SN9FdZV-vhs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/finetune_LLM'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLF3_kCn-xZS"
   },
   "outputs": [],
   "source": [
    "dataset[\"test\"]['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89VWF7ED-y_9"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "file_name = []\n",
    "infer_time = []\n",
    "for i in range(len(dataset[\"test\"])):\n",
    "    #if i == 14:\n",
    "    eval_prompt = prompt_func(i)\n",
    "    #print(eval_prompt)\n",
    "    file_name.append(dataset[\"test\"]['filename'][i])\n",
    "    #print(dataset[\"test\"]['filename'][i].split(\".\")[-2])\n",
    "    path = \"/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/test/\" #\"/home/necuser/sdp/Classification/LLM/Finetuining/130624/data/result/test\"\n",
    "    csv_file = dataset[\"test\"]['filename'][i].split(\".\")[-2] + \".csv\"\n",
    "    csv_path = os.path.join(path, csv_file)\n",
    "    print(csv_path)\n",
    "    start_time = time.time()\n",
    "    df_long = ft_model_func(eval_prompt)\n",
    "    end_time = time.time()\n",
    "    if not df_long.empty:\n",
    "        df_long.to_csv(csv_path, index=False)\n",
    "        infer_time.append(round(end_time-start_time, 2))\n",
    "        if i > 10:\n",
    "            break\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[187.43,\n",
       " 31.47,\n",
       " 188.25,\n",
       " 187.49,\n",
       " 186.77,\n",
       " 188.45,\n",
       " 187.07,\n",
       " 187.16,\n",
       " 187.81,\n",
       " 188.8,\n",
       " 188.2,\n",
       " 188.99]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Notes_text_shrishti_chunk_7024.txt',\n",
       " 'Notes_text_shrishti_chunk_9370.txt',\n",
       " 'Notes_text_shrishti_chunk_3347.txt',\n",
       " 'Notes_text_shrishti_chunk_6411.txt',\n",
       " 'Notes_text_shrishti_chunk_6434.txt',\n",
       " 'Notes_text_shrishti_chunk_4285.txt',\n",
       " 'Notes_text_shrishti_chunk_5829.txt',\n",
       " 'Notes_text_shrishti_chunk_7582.txt',\n",
       " 'Notes_text_shrishti_chunk_3564.txt',\n",
       " 'Notes_text_shrishti_chunk_9226.txt',\n",
       " 'Notes_text_shrishti_chunk_7283.txt',\n",
       " 'Notes_text_shrishti_chunk_6665.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "b4xS4P1w-1HG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/test/Notes_text_shrishti_chunk_6434.csv\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/ground_truth/Notes_text_shrishti_chunk_6434.csv\n",
      "Data from merged data: Notes_text_shrishti_chunk_6434.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    recipere       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "\n",
      "\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/test/Notes_text_shrishti_chunk_9370.csv\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/ground_truth/Notes_text_shrishti_chunk_9370.csv\n",
      "Data from merged data: Notes_text_shrishti_chunk_9370.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " examination       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "\n",
      "\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/test/Notes_text_shrishti_chunk_7024.csv\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/ground_truth/Notes_text_shrishti_chunk_7024.csv\n",
      "Data from merged data: Notes_text_shrishti_chunk_7024.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  complaints       1.00      1.00      1.00         1\n",
      "   diagnoses       1.00      1.00      1.00         1\n",
      " examination       1.00      1.00      1.00         1\n",
      "        plan       1.00      1.00      1.00         1\n",
      "    recipere       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "\n",
      "\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/test/Notes_text_shrishti_chunk_3347.csv\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/ground_truth/Notes_text_shrishti_chunk_3347.csv\n",
      "Data from merged data: Notes_text_shrishti_chunk_3347.csv:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 complaints       1.00      1.00      1.00         1\n",
      "                examination       1.00      1.00      1.00         1\n",
      "history of previous illness       0.00      0.00      0.00         3\n",
      "history_of_previous_illness       0.00      0.00      0.00         0\n",
      "             investigations       1.00      1.00      1.00         1\n",
      "                       plan       1.00      1.00      1.00         2\n",
      "                   recipere       1.00      1.00      1.00         1\n",
      "\n",
      "                   accuracy                           0.67         9\n",
      "                  macro avg       0.71      0.71      0.71         9\n",
      "               weighted avg       0.67      0.67      0.67         9\n",
      "\n",
      "\n",
      "\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/test/Notes_text_shrishti_chunk_7283.csv\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/ground_truth/Notes_text_shrishti_chunk_7283.csv\n",
      "Data from merged data: Notes_text_shrishti_chunk_7283.csv:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 complaints       1.00      1.00      1.00         1\n",
      "                examination       1.00      1.00      1.00         1\n",
      "history of previous illness       0.00      0.00      0.00         1\n",
      "history_of_previous_illness       0.00      0.00      0.00         0\n",
      "                       plan       1.00      1.00      1.00         3\n",
      "                   recipere       1.00      1.00      1.00         2\n",
      "\n",
      "                   accuracy                           0.88         8\n",
      "                  macro avg       0.67      0.67      0.67         8\n",
      "               weighted avg       0.88      0.88      0.88         8\n",
      "\n",
      "\n",
      "\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/test/Notes_text_shrishti_chunk_5829.csv\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/ground_truth/Notes_text_shrishti_chunk_5829.csv\n",
      "Data from merged data: Notes_text_shrishti_chunk_5829.csv:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 complaints       0.00      0.00      0.00         1\n",
      "                examination       1.00      1.00      1.00         1\n",
      "history of previous illness       0.00      0.00      0.00         1\n",
      "history_of_previous_illness       0.00      0.00      0.00         0\n",
      "                       plan       1.00      1.00      1.00         1\n",
      "                   recipere       1.00      1.00      1.00         1\n",
      "\n",
      "                   accuracy                           0.60         5\n",
      "                  macro avg       0.50      0.50      0.50         5\n",
      "               weighted avg       0.60      0.60      0.60         5\n",
      "\n",
      "\n",
      "\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/test/Notes_text_shrishti_chunk_9226.csv\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/ground_truth/Notes_text_shrishti_chunk_9226.csv\n",
      "Data from merged data: Notes_text_shrishti_chunk_9226.csv:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                       Null       0.00      0.00      0.00         0\n",
      "                 complaints       1.00      1.00      1.00         4\n",
      "                  diagnoses       0.00      0.00      0.00         1\n",
      "                examination       1.00      1.00      1.00         2\n",
      "history of previous illness       0.00      0.00      0.00         1\n",
      "history_of_previous_illness       0.00      0.00      0.00         0\n",
      "                   recipere       1.00      1.00      1.00         2\n",
      "\n",
      "                   accuracy                           0.80        10\n",
      "                  macro avg       0.43      0.43      0.43        10\n",
      "               weighted avg       0.80      0.80      0.80        10\n",
      "\n",
      "\n",
      "\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/test/Notes_text_shrishti_chunk_6411.csv\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/ground_truth/Notes_text_shrishti_chunk_6411.csv\n",
      "Data from merged data: Notes_text_shrishti_chunk_6411.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        plan       1.00      1.00      1.00         1\n",
      "    recipere       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "\n",
      "\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/test/Notes_text_shrishti_chunk_6665.csv\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/ground_truth/Notes_text_shrishti_chunk_6665.csv\n",
      "Data from merged data: Notes_text_shrishti_chunk_6665.csv:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                examination       1.00      1.00      1.00         1\n",
      "history of previous illness       0.00      0.00      0.00         1\n",
      "history_of_previous_illness       0.00      0.00      0.00         0\n",
      "                       plan       1.00      1.00      1.00         2\n",
      "                   recipere       1.00      1.00      1.00         3\n",
      "\n",
      "                   accuracy                           0.86         7\n",
      "                  macro avg       0.60      0.60      0.60         7\n",
      "               weighted avg       0.86      0.86      0.86         7\n",
      "\n",
      "\n",
      "\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/test/Notes_text_shrishti_chunk_7582.csv\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/ground_truth/Notes_text_shrishti_chunk_7582.csv\n",
      "Data from merged data: Notes_text_shrishti_chunk_7582.csv:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                       Null       0.00      0.00      0.00         0\n",
      "                 complaints       1.00      1.00      1.00         1\n",
      "                  diagnoses       0.00      0.00      0.00         1\n",
      "                examination       1.00      1.00      1.00         1\n",
      "history of previous illness       0.00      0.00      0.00         4\n",
      "history_of_previous_illness       0.00      0.00      0.00         0\n",
      "             investigations       1.00      1.00      1.00         2\n",
      "                   recipere       1.00      1.00      1.00         1\n",
      "\n",
      "                   accuracy                           0.50        10\n",
      "                  macro avg       0.50      0.50      0.50        10\n",
      "               weighted avg       0.50      0.50      0.50        10\n",
      "\n",
      "\n",
      "\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/test/Notes_text_shrishti_chunk_3564.csv\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/ground_truth/Notes_text_shrishti_chunk_3564.csv\n",
      "Data from merged data: Notes_text_shrishti_chunk_3564.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Null       0.00      0.00      0.00         0\n",
      "  complaints       1.00      1.00      1.00         2\n",
      "   diagnoses       0.00      0.00      0.00         1\n",
      "        plan       1.00      1.00      1.00         1\n",
      "    recipere       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.60      0.60      0.60         5\n",
      "weighted avg       0.80      0.80      0.80         5\n",
      "\n",
      "\n",
      "\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/test/Notes_text_shrishti_chunk_4285.csv\n",
      "/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/ground_truth/Notes_text_shrishti_chunk_4285.csv\n",
      "Data from merged data: Notes_text_shrishti_chunk_4285.csv:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                       Null       0.00      0.00      0.00         0\n",
      "                 complaints       0.50      0.50      0.50         2\n",
      "                  diagnoses       0.00      0.00      0.00         2\n",
      "                examination       1.00      1.00      1.00         2\n",
      "history of previous illness       0.00      0.00      0.00         4\n",
      "history_of_previous_illness       0.00      0.00      0.00         0\n",
      "             investigations       1.00      1.00      1.00         1\n",
      "                       plan       1.00      1.00      1.00         3\n",
      "                   recipere       1.00      1.00      1.00         2\n",
      "\n",
      "                   accuracy                           0.56        16\n",
      "                  macro avg       0.50      0.50      0.50        16\n",
      "               weighted avg       0.56      0.56      0.56        16\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/python39/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "merged_folder_path = \"/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/merged/\" #\"/home/necuser/sdp/Classification/LLM/Finetuining/130624/data/result/merged\"\n",
    "gen_folder_path = \"/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/test/\" #\"/home/necuser/sdp/Classification/LLM/Finetuining/130624/data/result/test\"\n",
    "gt_folder_path = \"/home/jupyter/finetune_LLM/finetuned_LLM_result/shrishti/ground_truth/\" #\"/home/necuser/sdp/Classification/LLM/Finetuining/130624/data/result/ground_truth\"\n",
    "\n",
    "files = os.listdir(gen_folder_path) #os.listdir(folder_path)\n",
    "\n",
    "csv_files = [file for file in files if file.endswith('.csv')]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    gen_file_path = os.path.join(gen_folder_path, csv_file) #os.path.join(folder_path, csv_file)\n",
    "    print(gen_file_path)\n",
    "    gt_file_path = os.path.join(gt_folder_path, csv_file)\n",
    "    print(gt_file_path)\n",
    "    gen_df = pd.read_csv(gen_file_path)\n",
    "    if os.path.isfile(gt_file_path):\n",
    "        gt_df = pd.read_csv(gt_file_path)\n",
    "        # Apply lower case to the 'Text' column\n",
    "        gt_df['Label'] = gt_df['Label'].str.lower()\n",
    "        gt_df['Text'] = gt_df['Text'].str.lower()\n",
    "        gen_df['Text'] = gen_df['Text'].str.lower()\n",
    "        # Merging the DataFrames on the 'Text' column using a left join\n",
    "        merged_df = pd.merge(gt_df, gen_df, on='Text', how='left', suffixes=('_gt_df', '_gen_df'))\n",
    "\n",
    "        print(f\"Data from merged data: {csv_file}:\")\n",
    "        # Replace NaN values with \"Null\"\n",
    "        merged_df.fillna(\"Null\", inplace=True)\n",
    "\n",
    "        merged_csv_path = os.path.join(merged_folder_path, csv_file)\n",
    "        merged_df.to_csv(merged_csv_path, index=False)\n",
    "        # Display the merged DataFrame\n",
    "        #print(merged_df)\n",
    "\n",
    "        print(classification_report(merged_df['Label_gt_df'], merged_df['Label_gen_df']))\n",
    "\n",
    "    #print(f\"Data from llm generated: {csv_file}:\")\n",
    "    #print(gen_df)\n",
    "    #print(f\"Data from ground truth: {csv_file}:\")\n",
    "    #print(gt_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PhcyzeO-7IL"
   },
   "outputs": [],
   "source": [
    "#Missing Information\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "merged_folder_path = \"/home/necuser/sdp/Classification/LLM/Finetuining/130624/data/result/merged/\"\n",
    "\n",
    "files = os.listdir(merged_folder_path)\n",
    "\n",
    "csv_files = [file for file in files if file.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(merged_folder_path, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "#print(merged_df)\n",
    "\n",
    "null_count = merged_df[merged_df['Label_gen_df'] == 'Null'].shape[0]\n",
    "\n",
    "print(f\"Number of rows where 'Label_gen_df' is 'Null': {null_count}\")\n",
    "\n",
    "null_percentage = (null_count / len(merged_df)) * 100\n",
    "\n",
    "print(f\"Percentage of 'Null' values in 'Label_gen_df': {null_percentage:.2f}%\")\n",
    "\n",
    "merged_df_cleaned = merged_df[merged_df['Label_gen_df'] != 'Null']\n",
    "\n",
    "print(\"\\nCleaned DataFrame:\")\n",
    "#print(merged_df_cleaned)\n",
    "print(classification_report(merged_df_cleaned['Label_gt_df'], merged_df_cleaned['Label_gen_df']))\n",
    "\n",
    "#merged_df_cleaned.to_csv(os.path.join(merged_folder_path, \"cleaned_merged_result.csv\", index=False)\n",
    "\n",
    "merged_df[merged_df['Label_gen_df'] == 'prior_treatment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwfZk4_T-95E"
   },
   "outputs": [],
   "source": [
    "merged_df[merged_df['Label_gen_df'] == 'Null']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgUIdEbC--ai"
   },
   "outputs": [],
   "source": [
    "#observations\n",
    "1. Hallucination - More Training data, steps, parameters tweaking\n",
    "2. Missing data(input notes) during inerenceing - More Training data, steps, parameters tweaking\n",
    "3. Generated output need post processing\n",
    "4. Need to test on all training and test data\n",
    "5. Inference Time is around 40-50 secs - Explore methods for faster inferencing\n",
    "6. Chunk overlapping - Retraining\n",
    "7. Data Augmentation - Retraining\n",
    "6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olhzOl9Y_Apo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ksvmEkB_DP9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkDm8lFq_DTh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ee6kWx1_DXB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRPHJzUO_DbD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23NNAkNN_Ddw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcj3Hd4t_Die"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_tokens = ft_model.generate(\n",
    "        **model_input,\n",
    "        max_new_tokens=1100, #1048,\n",
    "        num_beams=5,\n",
    "        temperature=0.0,\n",
    "        top_k=10,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=2.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "\n",
    "#outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "#eval_tokenizer.batch_decode(generated_tokens)\n",
    "\n",
    "# Decode the outputs\n",
    "decoded_outputs = eval_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Print the decoded outputs\n",
    "for output in decoded_outputs:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwksqXuH_EIZ"
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "#ft_model.eval()\n",
    "#with torch.no_grad():\n",
    "#    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=1048)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mkdqs0TM_Hjl"
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "#ft_model.eval()\n",
    "#with torch.no_grad():\n",
    "#    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=1048)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1qr00et_JfE"
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "#ft_model.eval()\n",
    "#with torch.no_grad():\n",
    "#    generated_tokens = ft_model.generate(\n",
    "#        **model_input,\n",
    "#        max_new_tokens=1048,\n",
    "#        num_beams=5,\n",
    "#        temperature=0.0,\n",
    "#        top_k=10,\n",
    "#        top_p=0.9,\n",
    "#        repetition_penalty=2.0,\n",
    "#        early_stopping=True\n",
    "#    )\n",
    "#    print(eval_tokenizer.decode(generated_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFIDCmDZ_LNy"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_tokens = ft_model.generate(\n",
    "        **model_input,\n",
    "        max_new_tokens=1100, #1048,\n",
    "        num_beams=5,\n",
    "        temperature=0.0,\n",
    "        top_k=10,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=2.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "\n",
    "#outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "#eval_tokenizer.batch_decode(generated_tokens)\n",
    "\n",
    "# Decode the outputs\n",
    "decoded_outputs = eval_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Print the decoded outputs\n",
    "for output in decoded_outputs:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHF0tjQJ_Ngw"
   },
   "outputs": [],
   "source": [
    "# Print the decoded outputs\n",
    "#for output in decoded_outputs:\n",
    "#    print(output)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56EyEIJf_Pid"
   },
   "outputs": [],
   "source": [
    "#decoded_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAXdpAw-_RLn"
   },
   "outputs": [],
   "source": [
    "#import re\n",
    "# Extract information after \"### Classified:\"\n",
    "#regex_pattern = r'### Classified:\\s*(.+)'\n",
    "#classified_data = re.search(regex_pattern, decoded_outputs[0], re.DOTALL)\n",
    "\n",
    "#if classified_data:\n",
    "#    classified_info = classified_data.group(1).strip()  # Extract the match and strip extra whitespace\n",
    "#    print(classified_info)\n",
    "#else:\n",
    "#    print(\"No classified data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gc7f6Upf_TDY"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "# Regular expression to extract the JSON-like dictionary after \"### Classified:\"\n",
    "match = re.search(r'### Classified:\\s*({.*?})\\s*(?=\\n|$)', decoded_outputs[0], re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    classified_data = match.group(1)\n",
    "    print(\"Extracted Data:\", classified_data)\n",
    "\n",
    "    # Convert the string to a dictionary\n",
    "    classified_dict = json.loads(classified_data.replace(\"'\", '\"'))  # Replace single quotes with double quotes for valid JSON\n",
    "    print(\"Dictionary Format:\", classified_dict)\n",
    "\n",
    "    # Convert the dictionary to a DataFrame for better visualization or further processing\n",
    "    df = pd.DataFrame.from_dict({key: pd.Series(value) for key, value in classified_dict.items()})\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"No classified data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFA2Qy7m_VOP"
   },
   "outputs": [],
   "source": [
    " df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSs0GEgz_Xbp"
   },
   "outputs": [],
   "source": [
    "# Transform the DataFrame\n",
    "# 'value_vars' is optional if you want to transform all columns\n",
    "df_long = df.melt(var_name='Label', value_name='Text', value_vars=df.columns)\n",
    "\n",
    "# Remove rows where 'Text' is None or NaN (if you need to clean up missing data)\n",
    "df_long.dropna(subset=['Text'], inplace=True)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(\"\\nTransformed DataFrame:\")\n",
    "print(df_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtxAjoBQ_Zw6"
   },
   "outputs": [],
   "source": [
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-cFuuTW_bfL"
   },
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "df_long.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QknB60wC_eCv"
   },
   "outputs": [],
   "source": [
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rPfxO48_fmV"
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['notes'])):\n",
    "        text = f\"\"\"Classify the notes into below categories:\n",
    "                   'allergies',\n",
    "                   'chief_complaints',\n",
    "                   'diagnosis',\n",
    "                   'family_history',\n",
    "                   'history',\n",
    "                   'instructions_advice',\n",
    "                   'investigation_report',\n",
    "                   'investigations',\n",
    "                   'medicine_prescription',\n",
    "                   'observations_examinations',\n",
    "                   'patient_willingness_concent',\n",
    "                   'personal_history',\n",
    "                   'procedure_report',\n",
    "                   'referral',\n",
    "                   'social_history',\n",
    "                   'tolerance',\n",
    "                   'treatment_plan',\n",
    "                   'unclassified',\n",
    "                   'vitals'\n",
    "                    Output the classified data into json format\n",
    "\n",
    "\n",
    "        ### notes:\n",
    "        {example[\"notes\"]}\n",
    "\n",
    "        ### Classified:\n",
    "        {example[\"classified\"]}\n",
    "        \"\"\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpG4tG1NK4S6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python39",
   "name": "tf2-gpu.2-11.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m111"
  },
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
